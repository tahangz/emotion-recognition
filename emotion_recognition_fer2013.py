# -*- coding: utf-8 -*-
"""emotion_recognition_FER2013.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SNiDJMvBDUwmfuwZQ008upAZHyw8LkWx

# 1. Setup Environment
"""

# =============================================
# Standard Library Imports
# =============================================
import io
import time

# =============================================
# Third-Party Imports
# =============================================
# Numerical Computing
import numpy as np

# Computer Vision
import cv2
from PIL import Image

# Deep Learning
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (
    Conv2D,
    MaxPooling2D,
    Dense,
    Dropout,
    Flatten
)
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import (
    ModelCheckpoint,
    EarlyStopping
)

# Jupyter/Colab Specific
from IPython.display import display, Javascript, HTML
from google.colab.output import eval_js
from base64 import b64decode, b64encode

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Install required libraries
!pip install tensorflow opencv-python matplotlib numpy

"""# 1.1 Parameters"""

# Define parameters
IMG_SIZE = (48, 48)
BATCH_SIZE = 32

EPOCHS=20

"""# 2. Setup Kaggle API and Download Dataset

---


"""

# Setup Kaggle API
from google.colab import files
files.upload()  # Upload your kaggle.json file

# Move Kaggle API key to proper location
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# Download FER2013 dataset
!kaggle datasets download -d msambare/fer2013

# Unzip dataset
!unzip fer2013.zip -d fer2013

"""# 3. Preprocess Data"""

# Create data generators
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=15,
    zoom_range=0.2,
    horizontal_flip=True,
    validation_split=0.2
)

train_generator = train_datagen.flow_from_directory(
    'fer2013/train',
    target_size=IMG_SIZE,
    color_mode='grayscale',
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='training'
)

validation_generator = train_datagen.flow_from_directory(
    'fer2013/train',
    target_size=IMG_SIZE,
    color_mode='grayscale',
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='validation'
)

# Class labels
emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']

"""# 4. Build CNN Model"""

model = Sequential()

# First CNN layer
model.add(Conv2D(32, (3,3), activation='relu', input_shape=(48,48,1)))
model.add(MaxPooling2D((2,2)))
model.add(Dropout(0.25))

# Second CNN layer
model.add(Conv2D(64, (3,3), activation='relu'))
model.add(MaxPooling2D((2,2)))
model.add(Dropout(0.25))

# Third CNN layer
model.add(Conv2D(128, (3,3), activation='relu'))
model.add(MaxPooling2D((2,2)))
model.add(Dropout(0.25))

# Flatten and Dense layers
model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(7, activation='softmax'))

model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

"""# 5. Train Model"""

# Callbacks
checkpoint = ModelCheckpoint('best_model.h5',
                             monitor='val_accuracy',
                             save_best_only=True,
                             mode='max')
early_stop = EarlyStopping(monitor='val_loss', patience=5)

# Train
history = model.fit(
    train_generator,
    validation_data=validation_generator,
    epochs=EPOCHS,
    callbacks=[checkpoint, early_stop]
)

# Save final model to Drive
model.save('/content/drive/MyDrive/emotion_model.h5')

"""# 6. Real-Time Emotion Detection"""

# Load your saved model (ignore the warning - it's harmless)
model_path = '/content/drive/MyDrive/emotion_model.h5'
model = tf.keras.models.load_model(model_path)

# Load Haar Cascade for face detection
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

# Emotion labels
emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']

# JavaScript to create webcam stream
js = Javascript('''
    async function setupWebcam() {
        const video = document.createElement('video');
        const stream = await navigator.mediaDevices.getUserMedia({video: true});
        video.srcObject = stream;
        await video.play();

        // Set video size
        video.width = 640;
        video.height = 480;

        // Create canvas for capturing
        const canvas = document.createElement('canvas');
        canvas.width = video.width;
        canvas.height = video.height;

        // Append to document
        document.body.appendChild(video);
        document.body.appendChild(canvas);

        return {'video': true};
    }
''')

# Display JavaScript and setup webcam
display(js)
eval_js('setupWebcam()')

# Function to get frame from webcam
def get_frame():
    # JavaScript to capture frame and return as base64
    frame_data = eval_js('''
        async function capture() {
            const video = document.querySelector('video');
            const canvas = document.querySelector('canvas');
            const context = canvas.getContext('2d');
            context.drawImage(video, 0, 0, canvas.width, canvas.height);
            return canvas.toDataURL('image/jpeg', 0.8);
        }
        capture();
    ''')

    # Convert base64 to OpenCV image
    header, encoded = frame_data.split(",", 1)
    binary = b64decode(encoded)
    image = Image.open(io.BytesIO(binary))
    frame = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
    return frame

# Function to process frame
def process_frame(frame):
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(gray, 1.1, 5)

    for (x, y, w, h) in faces:
        try:
            roi_gray = gray[y:y+h, x:x+w]
            roi = cv2.resize(roi_gray, (48, 48))
            roi = roi.astype('float') / 255.0
            roi = np.expand_dims(roi, axis=-1)
            roi = np.expand_dims(roi, axis=0)

            prediction = model.predict(roi)[0]
            label = emotion_labels[np.argmax(prediction)]
            confidence = np.max(prediction)

            color = (0, 255, 0)
            cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)
            cv2.putText(frame, f"{label} ({confidence:.2f})",
                       (x, y-10), cv2.FONT_HERSHEY_SIMPLEX,
                       0.9, color, 2)
        except Exception as e:
            print(f"Error processing face: {e}")

    return frame

# Main loop
output = display(display_id=True)
print("Starting emotion detection... Press 'Stop' in Colab to exit")

try:
    while True:
        frame = get_frame()
        processed_frame = process_frame(frame)

        # Convert back to base64 for display
        _, buffer = cv2.imencode('.jpg', processed_frame)
        frame_b64 = b64encode(buffer).decode('utf-8')

        output.update(HTML(f"""
            <img src="data:image/jpeg;base64,{frame_b64}"
                 style="max-width: 640px; max-height: 480px">
        """))
        time.sleep(0.05)

except Exception as e:
    print(f"Stopped: {e}")
finally:
    # Clean up
    eval_js('''
        const video = document.querySelector('video');
        if (video && video.srcObject) {
            video.srcObject.getTracks().forEach(track => track.stop());
        }
    ''')
    print("Video capture stopped")